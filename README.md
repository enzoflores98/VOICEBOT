<h1 align="center"> üó£Ô∏è VoiceBot Project ü§ñ</h1>

<p align="center">
  <img src="https://github.com/user-attachments/assets/d4ab1a18-9311-4dee-971c-c33eea6f4b84" alt="Logo de VoiceBot en Oro" width="300">
</p>

## Equipo üë•

[Enzo Flores](https://github.com/enzoflores98)  
[Nicolas Barcia](https://github.com/NicoBar91)

## Instituci√≥n üéì

[Universidad Nacional de Lomas de Zamora](https://www.unlz.edu.ar/)  
[Facultad de Ingenieria](https://ingenieria.unlz.edu.ar/)


<h2 align="center">Introducci√≥n üß≠</h2>

VoiceBot es un brazo rob√≥tico de cuatro grados de libertad (4GL) el cual, utilizando visi√≥n artificial y reconocimiento de voz, es capaz tomar piezas distinguidas por forma y color para colocarlas en los dep√≥sitos ordenados. Su nombre es debido a que su principal funcionalidad es manejar las funciones de pick and place a trav√©s de comandos de voz.

<h2 align="center">Objetivos üéØ</h2>

- Creacion de un robot manipulador que sea controlable a trav√©s de comandos de voz.
- Integrar visi√≥n artificial para el reconocimiento de objetos.
  
<h2 align="center">Modo de uso üöÄ</h2>

### Preparaci√≥n
1.	Colocar las piezas en el √°rea de detecci√≥n.
2.	Verificar que los dep√≥sitos se encuentren en posici√≥n.
3.	Verificar conexi√≥n a fuentes de energ√≠a, Webcam, Arduino/USB y micr√≥fono activo.

### Funcionamiento
1. Ejecutar script main.py.
2. Dar las ordenes en lenguaje natural. Pueden darse varias ordenes de una vez y el robot sera capaz de reconocerlas y ejecutarlas.
El algoritmo ser√° capaz de reconocer su intenci√≥n aun cuando la instruccion no sea concisa.
3. Aguardar proceso de Pick & Place.
4. Realizar nueva orden.

<p align="center">
  <img src="voicebot.gif" width="600"/>
</p>

üìΩÔ∏è [Ver demo en video](demo.mp4)


<h2 align="center">Caracteristicas generales üìù</h2>


### Esquema del robot

<p align="center">
  <img src="https://github.com/user-attachments/assets/1dc38a6f-7934-4419-8af1-cd8d0cc62e7d" width="600">
</p>

- Longitud total del brazo extendido en horizontal: 381.21mm
- Longitud total del brazo extendido en vertical: 433.16mm

### Componentes

Los eslabones del brazo rob√≥tico fueron impresos en 3D con el material PLA. Las piezas, dep√≥sitos y apoyos tambi√©n fueron fabricados con de la misma forma y con el mismo material. 

Para completar la maqueta/prototipo; se encuentra un soporte met√°lico encargado de sostener la luz LED, necesaria para eliminar problem√°ticas relacionadas con la detecci√≥n de imagen, y la Webcam encargada de tomar la imagen desde arriba. Todo esto se encuentra apoyado sobre una plataforma de madera de tipo melamina.


<h2 align="center">Cinem√°tica ‚öôÔ∏è</h2>

El c√≥digo para esta resolucion fue realizado en Python y se encuentra en el archivo cinematica.py adjunto en este repositorio.
El problema cinematico fue descompuesto en las siguientes partes:

1. Obtenci√≥n de las coordenadas XY de cada objeto dentro de la zona de detecci√≥n.
2. Traducci√≥n de estos valores a las coordenadas XYZ donde dirigir el efector del robot, tomando la base del mismo como referencia.
3. Obtenci√≥n de las coordenadas articulares Q1, Q2, Q3 y Q4 para los motores.
4. Env√≠o de estos valores a los servomotores mediante la conexion python-arduino.

Uno de los par√°metros que recibe este c√≥digo como entrada, es el centro del objeto que debe ir a tomar. El c√≥digo realiza la conversi√≥n de ese centro, que esta referenciado a un v√©rtice de la zona de detecci√≥n, para transformarlo a una posici√≥n referenciada al origen del robot. 
Esto se puede interpretar en el siguiente esquema donde el eje coordenado UV es el correspondiente a los objetos y el XY al robot, adem√°s de c√≥mo se relacionan matem√°ticamente entre s√≠.

<p align="center">
  <img src="https://github.com/user-attachments/assets/6f52e379-fac0-4e89-943a-43ba4f0e97f4" width="700">
</p>

### Cinematica Directa

Para la obtencion de las ecuaciones correspondientes a la cinematica directa de este brazo r√≥botico, se utilizo el algoritmo de Denavit & Hartemberg. El mismo consiste en obtener la matriz de transformacion homogenea que relaciona la posicion y orientacion del efector del robot con el origen del sistema mediante un metodo sistematico que establece cuatro transformaciones basicas.

Aplicando este algoritmo, el esquema y la tabla de parametros DH resulta:
<p align="center">
  <img src="https://github.com/user-attachments/assets/8c25439c-7188-4f6c-984a-3722218e7ad9" width="700">
</p>

Las ecuaciones que relacion cada sistema de con el del eje anterior, resultan:

$$
A_{01} = R_z(Q_1) \cdot T_z(l_{1z}) \cdot T_x(l_{1x}) \cdot R_x(\alpha)
$$

$$
A_{12} = R_z(Q_2 + \alpha) \cdot T_x(l_2)
$$

$$
A_{23} = R_z(Q_3) \cdot T_x(l_3)
$$

$$
A_{34} = R_z(Q_4) \cdot T_x(l_4)
$$

Finalmente, la MTH:

$$
MTH = A_{01} \cdot A_{12} \cdot A_{23} \cdot A_{34}
$$

Finalmente, esta matriz contiene en su ultima columna, en las tres primeras filas la posicion Px, Py y Pz del efector:


$$
\begin{cases}
P_x = \dfrac{ \left( -12000 \cdot \sin(Q_2) - 12286 \cdot \sin(Q_2 + Q_3) - 13000 \cdot \sin(Q_2 + Q_3 + Q_4) + 835 \right) \cdot \cos(Q_1) }{100} \\\\
P_y = \dfrac{ \left( -12000 \cdot \sin(Q_2) - 12286 \cdot \sin(Q_2 + Q_3) - 13000 \cdot \sin(Q_2 + Q_3 + Q_4) + 835 \right) \cdot \sin(Q_1) }{100} \\\\
P_z = 120 \cdot \cos(Q_2) + \dfrac{6143}{50} \cdot \cos(Q_2 + Q_3) + 130 \cdot \cos(Q_2 + Q_3 + Q_4) + \dfrac{603}{10}
\end{cases}
$$

### Cinamatica Inversa

Las tres ecuaciones que nos brinda el modelo de cinematica directa, obtenidas de la ultima columna de la Mth, nos plantean un escenario con 3 ecuaciones y 4 incognitas. Es por esto que establecemos una cuarta ecuacion vinculada con la orientacion del efector y que depende de un parametro predefinido que es el angulo de la mu√±eca con respecto al plano horizontal.

Esta ecuacion es:

$$
\alpha = Q_4 - Q_2 + Q_3
$$

El angulo alfa lo definimos segun dise√±o, 30¬∞ se utilizan para el caso.

Con cuatro ecuaciones para cuatro incognitas, utilizamos Python para resolverla mediante metodos numerico con la libreria fsolve. 
Si bien la resolucion de este sistema nos puede brindar varias soluciones que matematicamente satisfacen nuestro sistema de ecuaciones, se requieren una serie de restricciones para que los valores sean coherentes con el esquema fisico del robot.
Asimismo, es necesario una traduccion de los angulos matematicos a los angulos de giro de los servomotores, que dependen de su inicial real. Por este motivo, el codigo cuenta con una funcion especial para traducir este offset.



<h2 align="center">Software üíª</h2>


Con excepci√≥n del control de los motores servos, que se realiza con un c√≥digo de Arduino, el c√≥digo del proyecto se encuentra realizado √≠ntegramente con Python.

El lineamiento principal fue resolver las diferentes partes en m√≥dulos seg√∫n funcionalidad para luego integrarlas en un archivo principal que consulte a cada fuente por separado seg√∫n necesidad. El diagrama de flujo utilizado para poder incorporar la interacci√≥n de las diferentes funciones del c√≥digo es el siguiente:

<p align="center">
  <img src="https://github.com/user-attachments/assets/d3656247-c3b4-49b3-86f7-290a1840dea6" width="700">
</p>

### Reconocimiento de voz

El script que controla este codigo es audio.py. Este modulo se consulta desde main.py, su funcion es realizar el procesamiento de la orden dictada por el usuario y devolver al flujo principal una cada de texto de tres palabras: FORMA COLOR DEPOSITO. 

Se utiliza la librer√≠a Pyaudio, para el control del flujo de datos de audio y OpenAI para la transcripci√≥n y procesamiento de la orden. Un paso fundamental en este paso es el "entendimiento" de la orden dictada por el usuario. La misma se recibe en forma de texto plano y a trav√©s de un prompt que utiliza el modelo "gpt-3.5-turbo" se obtiene la salida en forma cadena de texto.

Ejemplo de prompt:

```txt
Debes responder con exactamente tres cadenas de texto. La primera ser√° una forma, la segunda ser√° un color y la tercera es un n√∫mero.
El usuario mencionar√° o insinuar√° una forma y un color expl√≠cita o impl√≠citamente, adem√°s de un n√∫mero de dep√≥sito. Tu tarea ser√° interpretar y responder.
Tu respuesta tendr√° el formato 'FORMA' 'COLOR' 'NUMERO' (el n√∫mero deber√° responderse con un car√°cter num√©rico).
Las formas como respuesta deben ser: 'CILINDRO', 'CUBO', 'ETC'.
Los colores como respuesta deben ser: 'ROJO', 'NEGRO', 'ETC'.
Tu respuesta ir√° directo a los datos de entrada para mover un robot el cual tomar√° la forma, color y n√∫mero como instrucci√≥n. Por este motivo es importante que solo respondas con el formato indicado.
Como √∫nico caso excepcional en donde responder√°s algo que sea diferente, es cuando las instrucciones no tengan sentido o no brinden la informaci√≥n suficiente. En ese caso deber√°s responder 'Instrucci√≥n no reconocida'.
No agregues ning√∫n texto ni car√°cter adicional.
Ten en cuenta que la entrada puede no ser del todo expl√≠cita y en ese caso deber√°s entender qu√© es lo que el usuario quiere hacer.
```

Los pasos que realiza este script, entonces, son:

1.	Recibe una orden por voz utilizando el micr√≥fono. Esto ser√° un audio en donde se encuentre la frase dictada por el usuario
2.	La frase recibida ser√° transcripta a texto haciendo uso del modelo ‚Äúwhisper-1‚Äù de OpenAI
3.	El texto ser√° procesado por un prompt predefinido que utilizar√° el modelo "gpt-3.5-turbo" y devuelve una orden concisa como cadena de caracteres con tres palabras: FORMA COLOR DEPOSITO. Por ejemplo: texto de entrada < ‚ÄùQuiero mover la pieza de color rojo y con forma cil√≠ndrica al segundo deposito‚Äù > devolver√° la instrucci√≥n < ‚ÄùCILINDRO ROJO 2‚Äù >



### Visi√≥n Artificial

El script que se utiliza para el reconocimiento de los objetos es deteccion.py. 

Para este c√≥digo se utiliz√≥ la librer√≠a CV2 desarrollada por OpenCV. La delimitaci√≥n del √°rea de detecci√≥n se realiz√≥ con marcadores aruco:

<p align="center">
  <img src="https://github.com/user-attachments/assets/3ec0a26a-6d17-4fc2-bc7d-325de4ee0113" width="600">
</p>

Su documentaci√≥n se puede encontrar en el siguiente [link](https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html)

Estos marcadores son muy utilizados en el campo de la visi√≥n artificial dado que se pueden identificar f√°cilmente con su sistema de codificaci√≥n y establecer los bordes para brindar un marco de referencia absoluto a la c√°mara.

Durante el desarrollo de este proyecto nos encontramos algunos desaf√≠os respecto a la vision artificial. Uno de ellos es respecto a las sombras:

<p align="center">
  <img src="https://github.com/user-attachments/assets/d26b2941-0db1-4917-be2e-0c8edddc4d47" width="400">
</p>

Como se aprecia en la imagen; la sombra puede a√±adir, no solo distorsi√≥n a las medidas reales del objeto en cuanto a su centro, sino tambi√©n lineas que puedan interpretarse como lados del objeto y por lo tanto provocar un error de identificacion de la forma. 
Para no depender de la iluminacion del ambiente, la resolucion adoptada fue colocar una fuente de LED que proporcione luz continua. 



En este modulo del software se realiza la identificaci√≥n de cada una de las piezas situadas en el √°rea de detecci√≥n. Esta identificaci√≥n se constituye en:
- **Forma**: el algoritmo utilizado identifica la forma del objeto seg√∫n la cantidad de aristas observadas. Se brinda al c√≥digo una serie de condicionales para las formas conocidas y se hace una verificaci√≥n de cu√°l es la que se cumple.
- **Color**: esta basada en el modelo HSV que define un color seg√∫n su matiz, saturaci√≥n y brillo (valor). El c√≥digo establece una serie mascaras para determinar un rango predefinido para cada color identificable por nuestro software.
- **Coordenadas del centro de cada objeto**: dada la naturaleza regular de los objetos con los que se trabaja en el alcance de este proyecto, este se calcula de forma simple consider√°ndolo inscripto en un rect√°ngulo del cual se calcula su centro con base/2 para X y altura/2 para Y.

A este script se ingresa desde main.py con una frame capturado por la camara y se espera que devuelva un vector de vectores denominado centros_objetos. Cada vector contenido dentro de centros_objetos tendra la forma (shape, color, cx, cy), que refiera a forma, color, coordenada X y coordenada Y del centro del objeto.

### Script Principal

Este codigo es el encargado de manipular el flujo del programa orquestando las diferentes entradas y salidas de parametros para cada fuente. 
Adem√°s, en este c√≥digo se realiza el matcheo entre los objetos detectados por la vision artificial y la orden dictada por el usuario. Una vez encontrada la coincidencia entre ambos, se envia la orden al script de cinematica para terminar con el proceso de pick and place.

El diagrama de flujo del programa queda de la siguiente forma:
1.	M√≥dulo deteccion.py recibe como par√°metro la imagen a analizar y devuelve un vector que contiene la informaci√≥n de cada objeto detectado.
2.	M√≥dulo audio.py, se activa desde main.py y devuelve una cadena de tres palabras: FORMA COLOR DEPOSITO.
3.	El c√≥digo main.py realiza una b√∫squeda para cruzar la informaci√≥n de la orden con el vector de objetos detectados y devuelve una posici√≥n XY que es el centro del objeto que se debe tomar.
4.	El modulo cinem√°tica.py recibe esta coordenada XY y el n√∫mero de dep√≥sito. Realiza las ecuaciones para el calculo de las variables Q1, Q2, Q3 y Q4 que llevan el efector a destino para tomar el objeto y depositarlo en el recipiente correcto. 
5.	Reinicio del bucle.

<h2 align="center">Componentes electr√≥nicos ‚ö°</h2>

-	Arduino UNO R3
-	4 servomotores MG995R
-	1 servomotores SG95
-	Fuente de alimentaci√≥n 5V 10A
-	Capacitor electrol√≠tico 3300 ¬µF 16V





